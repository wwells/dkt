{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwells/dkt/blob/master/notebooks/torch_DKT_ww.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import os "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl0gTkjZN7YQ",
        "outputId": "dfdf1759-25e6-45e8-d42a-e5cfe5aae04c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Working Env Ready\n",
        "\n",
        "These sections are used to prepare and clone our recommender-infra repo.   Be sure not to check in any commits with your GITHUB_PAT.   \n",
        "\n",
        "Example usecase: \n",
        "https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d"
      ],
      "metadata": {
        "id": "UJ0wWUqVRf_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get our working env ready\n",
        "HOME = '/content/drive/MyDrive'\n",
        "GITHUB_DIR = HOME + '/Github'\n",
        "NDA_RECOMMENDER_INFRA = GITHUB_DIR + '/recommender-infra'\n",
        "\n",
        "if not os.path.exists(GITHUB_DIR):\n",
        "   os.makedirs(GITHUB_DIR)"
      ],
      "metadata": {
        "id": "nsEv_pqhOXhU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT:  DO NOT commit this notebook with your GITHUB_PAT checked in.   \n",
        "USERNAME = 'wwells'\n",
        "GITHUB_PAT = 'somestring'"
      ],
      "metadata": {
        "id": "Dpwpfk5tQKbc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $GITHUB_DIR\n",
        "!git clone https://$GITHUB_PAT@github.com/khan-nda/recommender-infra.git\n",
        "%cd $NDA_RECOMMENDER_INFRA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3G0J4n9PziX",
        "outputId": "22ba1492-7ed5-4cb4-fc67-b268deb5b59d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github\n",
            "Cloning into 'recommender-infra'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 6 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n",
            "/content/drive/MyDrive/Github/recommender-infra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YC0r9_1hOFTq",
        "outputId": "f01bbea4-7471-4983-813a-c1e1ecd155e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Github/recommender-infra'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking in any commits\n",
        "\n"
      ],
      "metadata": {
        "id": "HCoFUUa8R3My"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CZuz6-taSkAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "UzE3OWWgSkhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnT2Frl82SZE"
      },
      "outputs": [],
      "source": [
        "inter_df = pd.read_csv('gdrive/My Drive/interactions.csv', sep=',') # example data from HawkesKT repo \n",
        "inter_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# aggregate data by user \n",
        "max_step=50 # caps the sequences\n",
        "user_wise_dict = dict()\n",
        "cnt, n_inters = 0, 0\n",
        "for user, user_df in inter_df.groupby('user_id'):\n",
        "            df = user_df[:max_step]  # consider the first 50 interactions\n",
        "            user_wise_dict[cnt] = {\n",
        "                'user_id': user,\n",
        "                'skill_seq': df['skill_id'].values.tolist(),\n",
        "                'correct_seq': [round(x) for x in df['correct']],\n",
        "                'time_seq': df['timestamp'].values.tolist(),\n",
        "                'problem_seq': df['problem_id'].values.tolist()\n",
        "            }\n",
        "            cnt += 1\n",
        "            n_inters += len(df)\n",
        "user_seq_df = pd.DataFrame.from_dict(user_wise_dict, orient='index')\n",
        "user_seq_df # this is the same as our seq step\n",
        "\n",
        "n_users = max(inter_df['user_id'].values) + 1\n",
        "n_skills = max(inter_df['skill_id']) + 1\n",
        "n_problems = max(inter_df['problem_id']) + 1\n",
        "user_seq_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "vREPzQWF5dJ8",
        "outputId": "2fabedec-b4e0-4c42-e182-1a33d1c4c29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id              skill_seq            correct_seq  \\\n",
              "0        0     [0, 0, 0, 1, 1, 1]     [1, 1, 1, 0, 0, 1]   \n",
              "1        1  [0, 0, 0, 0, 1, 1, 1]  [1, 1, 1, 1, 0, 1, 0]   \n",
              "\n",
              "                          time_seq            problem_seq  \n",
              "0      [0, 80, 129, 130, 220, 260]     [0, 1, 2, 3, 4, 5]  \n",
              "1  [0, 40, 80, 120, 200, 250, 280]  [0, 1, 2, 3, 4, 5, 6]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9e9c06a-6a1f-423f-bdcd-e9ce8c7623b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>skill_seq</th>\n",
              "      <th>correct_seq</th>\n",
              "      <th>time_seq</th>\n",
              "      <th>problem_seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 0, 0, 1]</td>\n",
              "      <td>[0, 80, 129, 130, 220, 260]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 0, 1, 0]</td>\n",
              "      <td>[0, 40, 80, 120, 200, 250, 280]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9e9c06a-6a1f-423f-bdcd-e9ce8c7623b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d9e9c06a-6a1f-423f-bdcd-e9ce8c7623b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d9e9c06a-6a1f-423f-bdcd-e9ce8c7623b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split, although more like setup for k-fold cross-v\n",
        "# data_df['dev'] is validation data\n",
        "# here we take a fifth of the available data and reserve it as test\n",
        "# then we take 10% of the remaining data as validation \n",
        "# the rest is training data. \n",
        "# all this is saved as \n",
        "k_fold=5\n",
        "data_df = {\n",
        "            'train': pd.DataFrame(), 'dev': pd.DataFrame(), 'test': pd.DataFrame()\n",
        "        }\n",
        "\n",
        "# def gen_fold_data(self, k):\n",
        "# assert k < k_fold\n",
        "k=0\n",
        "n_examples = len(user_seq_df)\n",
        "fold_size = math.ceil(n_examples / k_fold) # say we have 100 examples then this is 100/5 = 20 rows in each fold\n",
        "fold_begin = k * fold_size # ??\n",
        "fold_end = min((k + 1) * fold_size, n_examples)\n",
        "data_df['test'] = user_seq_df.iloc[fold_begin:fold_end]\n",
        "residual_df = pd.concat([user_seq_df.iloc[0:fold_begin], user_seq_df.iloc[fold_end:n_examples]])\n",
        "dev_size = int(0.1 * len(residual_df))\n",
        "dev_indices = np.random.choice(residual_df.index, dev_size, replace=False)  # random\n",
        "data_df['dev'] = user_seq_df.iloc[dev_indices]\n",
        "data_df['train'] = residual_df.drop(dev_indices)\n",
        "#logging.info('# Train: {}, # Dev: {}, # Test: {}'.format(len(data_df['train']), len(data_df['dev']), len(data_df['test'])))\n",
        "# so the above splits the two rows of seq data into one row per test, one train and 0 for dev"
      ],
      "metadata": {
        "id": "BpwotlNw-4hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch_end = min(len(data), batch_start + batch_size)\n",
        "#real_batch_size = batch_end - batch_start\n",
        "# ignoring all batch stuff for now \n",
        "\n",
        "# padding function \n",
        "def pad_lst(lst, value=0, dtype=np.int64):\n",
        "    inner_max_len = max(map(len, lst))\n",
        "    result = np.ones([len(lst), inner_max_len], dtype) * value\n",
        "    for i, row in enumerate(lst):\n",
        "        for j, val in enumerate(row):\n",
        "            result[i][j] = val\n",
        "    return result\n",
        "\n",
        "# data setup\n",
        "user_ids = user_seq_df['user_id'].values\n",
        "user_seqs = user_seq_df['skill_seq'].values\n",
        "label_seqs = user_seq_df['correct_seq'].values\n",
        "\n",
        "lengths = np.array(list(map(lambda lst: len(lst), user_seqs)))\n",
        "indice = np.array(np.argsort(lengths, axis=-1)[::-1])\n",
        "inverse_indice = np.zeros_like(indice)\n",
        "for i, idx in enumerate(indice):\n",
        "        inverse_indice[idx] = i\n",
        "\n",
        "feed_dict = {\n",
        "            'user_id': torch.from_numpy(user_ids[indice]),\n",
        "            'skill_seq': torch.from_numpy(pad_lst(user_seqs[indice])),    # [batch_size, num of items to predict]\n",
        "            'label_seq': torch.from_numpy(pad_lst(label_seqs[indice])),   # [batch_size, num of items to predict]\n",
        "            'length': torch.from_numpy(lengths[indice]),                        # [batch_size]\n",
        "            'inverse_indice': torch.from_numpy(inverse_indice),\n",
        "            'indice': torch.from_numpy(indice)\n",
        "        }\n",
        "\n",
        "feed_dict\n",
        "# feed dict is an input to the model \n",
        "# we have user id tenosr, \n",
        "# then skill_sequence tensor - which is padded with 0 to the longest sequence (here it's 7)\n",
        "# then we have label tensor, which is again length 7 each "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY_qjrUhB1nA",
        "outputId": "0e520f92-22bb-43f6-c5b4-21c44dd50e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_id': tensor([1, 0]), 'skill_seq': tensor([[0, 0, 0, 0, 1, 1, 1],\n",
              "         [0, 0, 0, 1, 1, 1, 0]]), 'label_seq': tensor([[1, 1, 1, 1, 0, 1, 0],\n",
              "         [1, 1, 1, 0, 0, 1, 0]]), 'length': tensor([7, 6]), 'inverse_indice': tensor([1, 0]), 'indice': tensor([1, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the model \n",
        "skill_num=n_skills\n",
        "emb_size=10\n",
        "hidden_size=10\n",
        "num_layer=1\n",
        "\n",
        "skill_embeddings = torch.nn.Embedding(skill_num * 2, emb_size) # embedding layer is skill_num*2 (which is feature length) by emb dim\n",
        "rnn = torch.nn.LSTM(\n",
        "            input_size=emb_size, hidden_size=hidden_size, batch_first=True,\n",
        "            num_layers=num_layer\n",
        "   )\n",
        "out = torch.nn.Linear(hidden_size, skill_num)\n",
        "loss_function = torch.nn.BCELoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "WxX3zyHR6025"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward_pass\n",
        "# in earlier step we define three layers:\n",
        "  # 1 embedding layer with as many rows as skills*2 (feature_length) and as many columns as emb dims\n",
        "  # LSTM Layer with input of emb dim columns, num of hidden units, layers\n",
        "  # output linear layer that as input takes 10 cols from LSTM layer, outputs as many columns as skills (here 2) I think (at least out dim =2)\n",
        "\n",
        "seq_sorted = feed_dict['skill_seq']     # [batch_size, history_max]\n",
        "labels_sorted = feed_dict['label_seq']  # [batch_size, history_max]\n",
        "lengths = feed_dict['length']           # [batch_size]\n",
        "\n",
        "# this is the feature we had before, where skill_id*2+correct\n",
        "# so this is passing the feature to the embeding layer that's size feature_length, emd dimensions\n",
        "# so very similar to the step where we one hot encoded the feature. \n",
        "# each user sequence is now converted to an embedding but here we get 10 dimensional embedding for each step in the sequence.\n",
        "embed_history_i = skill_embeddings(seq_sorted + labels_sorted * skill_num) # 7 X 10 embedings for two tensors (two users) \n",
        "\n",
        "# packing a tensor is converting several tensors into 1 by interleaving elements of both tensors, length-1 here takes 1st to n-1 elements, so time shifting piece \n",
        "embed_history_i_packed = torch.nn.utils.rnn.pack_padded_sequence(embed_history_i, lengths - 1, batch_first=True) # output here is 11 embeding vectors, \n",
        "# outpup is 11 embeding vectors, 64 dims each, Hidden layer is 4 X 64\n",
        "# it's 11 vectors because we took two tensors of 7 and 6 length, removed the last element and then packed into 1 so 6+5=11\n",
        "output, hidden = rnn(embed_history_i_packed, None) \n",
        "\n",
        "# output is 6 X 64 vectors, two tensors,second tensor last row is 0,since we had 6 and 5 elem input \n",
        "# I think pad_packed_sequence reverses the procedure from pack_padded_sequence()\n",
        "output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True) \n",
        "# 2 tensors, each 6X2, out() is the Linear dense layer, I think here we have two values for each student, each sequence step, likely logits\n",
        "pred_vector = out(output) \n",
        "\n",
        "# this takes skill sequences from 2nd element to last, so timeshifted. SO these are also length 6 each like the output of pred_vector\n",
        "target_item = seq_sorted[:, 1:] \n",
        "\n",
        "# gather creates a new tensor by picking values from input tensor based on index values provided, these index row or col values of input depending on dim\n",
        "# so here we have prediction tensor that's 6 by 2, index comes from unsqueezing (reshaping) 2x6 skill seq tensor along column dim.\n",
        "# index ends up looking like this for one seq (6 rows, 1 col) \n",
        " #  [[0],\n",
        " #   [0],\n",
        " #   [0],\n",
        " #   [1],\n",
        " #   [1],\n",
        " #   [1]] \n",
        "# we create a new tensor by picking a value from pred_vector tensor that match the index and then squeezing back into 2x6 shape  \n",
        "prediction_sorted = torch.gather(pred_vector, dim=-1, index=target_item.unsqueeze(dim=-1)).squeeze(dim=-1) # need to understand the squeez stuff, but this gives two tensors 6 vals each \n",
        "label = labels_sorted[:, 1:] # this is also timeshifted correct \n",
        "\n",
        "prediction_sorted = torch.sigmoid(prediction_sorted)\n",
        "        \n",
        "prediction = prediction_sorted[feed_dict['inverse_indice']]\n",
        "label = label[feed_dict['inverse_indice']].double()\n",
        "\n",
        "out_dict = {'prediction': prediction, 'label': label}\n",
        "# # embed_history_i # two tensors 7 x 64 each. (7 is the sequence length, so each sequence element is converted into 64 emb)\n",
        "# # seq_sorted+labels_sorted*2\n",
        "# embed_history_i_packed\n",
        "# target item is 2 by 6 tensor (this is just skill seq), unsqueeze reshapes it into 2 by 6 rows by 1 column (sort of wide to long), used as index in gather()\n"
      ],
      "metadata": {
        "id": "qJHpAC_v8McA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now loss function\n",
        "\n",
        "indice = feed_dict['indice']\n",
        "lengths = feed_dict['length'] - 1\n",
        "predictions, labels = out_dict['prediction'][indice], out_dict['label'][indice]\n",
        "predictions = torch.nn.utils.rnn.pack_padded_sequence(predictions, lengths, batch_first=True).data\n",
        "labels = torch.nn.utils.rnn.pack_padded_sequence(labels, lengths, batch_first=True).data\n",
        "loss_function(predictions.float(), labels.float())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Q372xg-bnN",
        "outputId": "fe981131-796b-417f-d398-09a4b39f0481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7479, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### FULL CODE FOR LSTM DKT BASED ON HawkesKT REPO \n",
        "\n",
        "# Read data \n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch \n",
        "import math\n",
        "\n",
        "inter_df = pd.read_csv('gdrive/My Drive/ka_lstm_df_big.txt', sep=',', header=None) # example data from HawkesKT repo \n",
        "inter_df.columns=['user_id', 'skill_id', 'correct']\n",
        "\n",
        "# prepare sequence data\n",
        "max_step=100 # caps the sequences\n",
        "user_wise_dict = dict()\n",
        "cnt, n_inters = 0, 0\n",
        "for user, user_df in inter_df.groupby('user_id'):\n",
        "            df = user_df[:max_step]  # consider the first 50 interactions\n",
        "            user_wise_dict[cnt] = {\n",
        "                'user_id': user,\n",
        "                'skill_seq': df['skill_id'].values.tolist(),\n",
        "                'correct_seq': [round(x) for x in df['correct']]\n",
        "            }\n",
        "            cnt += 1\n",
        "            n_inters += len(df)\n",
        "user_seq_df = pd.DataFrame.from_dict(user_wise_dict, orient='index')\n",
        "user_seq_df # this is the same as our seq step\n",
        "\n",
        "n_users = max(inter_df['user_id'].values) + 1\n",
        "n_skills = max(inter_df['skill_id']) + 1\n",
        "user_seq_df['seq_length'] = user_seq_df['correct_seq'].apply(len)\n",
        "user_seq_df=user_seq_df[user_seq_df.seq_length > 3] # filter out short sequences\n",
        "\n",
        "# set up feed_dict dataset \n",
        "# padding function \n",
        "def pad_lst(lst, value=0, dtype=np.int64):\n",
        "    inner_max_len = max(map(len, lst))\n",
        "    result = np.ones([len(lst), inner_max_len], dtype) * value\n",
        "    for i, row in enumerate(lst):\n",
        "        for j, val in enumerate(row):\n",
        "            result[i][j] = val\n",
        "    return result\n",
        "\n",
        "# data setup\n",
        "user_ids = user_seq_df['user_id'].values\n",
        "user_seqs = user_seq_df['skill_seq'].values\n",
        "label_seqs = user_seq_df['correct_seq'].values\n",
        "\n",
        "lengths = np.array(list(map(lambda lst: len(lst), user_seqs))) # same as seq_length above for each user \n",
        "# next we add an index by length\n",
        "# argsort returns indices that would sort an array, but the indices are in the same order as original array \n",
        "# [::2] is called Slice notation it means <start_index> <end_index> <by>, eg start at 0th index, end at last index, by 2 (every second element)\n",
        "# [::-1] -1 reverses the array, so start with the 0th, to last, by 1 from back \n",
        "indice = np.array(np.argsort(lengths, axis=-1)[::-1]) \n",
        "inverse_indice = np.zeros_like(indice) # array of zeros same shape as indice\n",
        "for i, idx in enumerate(indice):\n",
        "        inverse_indice[idx] = i\n",
        "\n",
        "feed_dict = {\n",
        "            'user_id': torch.from_numpy(user_ids[indice]), # this is ordred by longest sequence?\n",
        "            'skill_seq': torch.from_numpy(pad_lst(user_seqs[indice])),    # [batch_size, num of items to predict]\n",
        "            'label_seq': torch.from_numpy(pad_lst(label_seqs[indice])),   # [batch_size, num of items to predict]\n",
        "            'length': torch.from_numpy(lengths[indice]),                        # [batch_size]\n",
        "            'inverse_indice': torch.from_numpy(inverse_indice),\n",
        "            'indice': torch.from_numpy(indice)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxXiLGq1SEbs",
        "outputId": "99d3605a-b111-4fff-8fd3-841083b820af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# At this point we are done reading in data, transforming it, and running the forward pass of the model. \n",
        "# now need to figure out train and predict \n",
        "# good reference on steps - https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
        "\n",
        "# define the device\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# define the model \n",
        "\n",
        "class dkt_model(torch.nn.Module):\n",
        "  def __init__(self, emb_size, skill_num, hidden_size, num_layer):\n",
        "   super(dkt_model, self).__init__()\n",
        "\n",
        "   # define params\n",
        "   self.skill_num=n_skills\n",
        "   self.emb_size=emb_size\n",
        "   self.hidden_size=hidden_size\n",
        "   self.num_layer=num_layer\n",
        "   \n",
        "\n",
        "   # define the layers\n",
        "   self.skill_embeddings = torch.nn.Embedding(self.skill_num * 2, self.emb_size) # embedding layer is skill_num*2 (which is feature length) by emb dim\n",
        "   self.rnn = torch.nn.LSTM(\n",
        "            input_size=self.emb_size, hidden_size=self.hidden_size, batch_first=True,\n",
        "            num_layers=self.num_layer\n",
        "   )\n",
        "   self.out = torch.nn.Linear(self.hidden_size, self.skill_num)\n",
        "\n",
        "  # define the forward pass\n",
        "  #  1) get embeddings, 2) then pack them, 3) then pass through lstm layer, 4) unpack, 5) then dense layer\n",
        "  def forward(self, feed_dict):\n",
        "    seq_sorted = feed_dict['skill_seq']     # [batch_size, history_max]\n",
        "    labels_sorted = feed_dict['label_seq']  # [batch_size, history_max]\n",
        "    lengths = feed_dict['length']           # [batch_size]\n",
        "\n",
        "    embed_history_i = self.skill_embeddings(seq_sorted + labels_sorted * self.skill_num)  \n",
        "    embed_history_i_packed = torch.nn.utils.rnn.pack_padded_sequence(embed_history_i, lengths - 1, batch_first=True)\n",
        "    output, hidden = self.rnn(embed_history_i_packed, None) \n",
        "    output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True) \n",
        "    pred_vector = self.out(output) \n",
        "\n",
        "    target_item = seq_sorted[:, 1:] # timeshifted sequence of exerciees \n",
        "    label = labels_sorted[:, 1:] # timeshifted sequence of labels\n",
        " \n",
        "    prediction_sorted = torch.gather(pred_vector, dim=-1, index=target_item.unsqueeze(dim=-1)).squeeze(dim=-1) \n",
        "    prediction_sorted = torch.sigmoid(prediction_sorted)\n",
        "    prediction = prediction_sorted[feed_dict['inverse_indice']]\n",
        "    label = label[feed_dict['inverse_indice']].double()\n",
        "    out_dict = {'prediction': prediction, 'label': label}\n",
        "    return out_dict\n",
        "\n",
        "    \n",
        "# Instantiate the model with paramters\n",
        "dktm = dkt_model(emb_size=64, skill_num=n_skills, hidden_size=64, num_layer=1)  \n",
        "dktm.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 6\n",
        "lr=0.001\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "optimizer = torch.optim.Adam(dktm.parameters(), lr=lr)\n",
        "loss_function = torch.nn.BCELoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "Co1X1q1LH8Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08w0oPGApUWj",
        "outputId": "6f9acbb9-8022-4b8b-b0cd-05d3628e093c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_id': tensor([3181, 6443, 3221,  ..., 3689, 9104, 3237]),\n",
              " 'skill_seq': tensor([[ 66,  66,  66,  ..., 123, 123,  20],\n",
              "         [ 83,  83,  83,  ..., 101, 101, 101],\n",
              "         [ 30,  30,  30,  ..., 148,  37,  37],\n",
              "         ...,\n",
              "         [ 67,  67,  67,  ...,   0,   0,   0],\n",
              "         [101, 101, 101,  ...,   0,   0,   0],\n",
              "         [ 15,  15,  15,  ...,   0,   0,   0]]),\n",
              " 'label_seq': tensor([[0, 1, 1,  ..., 1, 0, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [0, 1, 0,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'length': tensor([100, 100, 100,  ...,   4,   4,   4]),\n",
              " 'inverse_indice': tensor([2790, 4429, 5556,  ..., 2492, 7799, 7403]),\n",
              " 'indice': tensor([2949, 5996, 2983,  ..., 3425, 8489, 2998])}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tmp=dktm(feed_dict)\n",
        "tmp['label'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTPeMW8DrvsL",
        "outputId": "b8728318-38c3-4bcd-92b1-98cf4be9c14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8789, 99])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now define the training run\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    out_dict = dktm(feed_dict) # this step outputs the result of the forward pass (8799 X 99 dataset)\n",
        "    indice = feed_dict['indice']\n",
        "    lengths = feed_dict['length'] - 1\n",
        "    predictions, labels = out_dict['prediction'][indice], out_dict['label'][indice]\n",
        "    predictions = torch.nn.utils.rnn.pack_padded_sequence(predictions, lengths, batch_first=True).data\n",
        "    labels = torch.nn.utils.rnn.pack_padded_sequence(labels, lengths, batch_first=True).data\n",
        "    loss=loss_function(predictions.float(), labels.float())\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # updates the weights\n",
        "    \n",
        "    if epoch%2 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh_1wBl0J7QX",
        "outputId": "7d9f883a-335c-4669-d0c0-978f18f2e38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/6............. Loss: 0.6924\n",
            "Epoch: 4/6............. Loss: 0.6817\n",
            "Epoch: 6/6............. Loss: 0.6716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# w = list(dktm.parameters())\n",
        "# len(w) \n",
        "\n",
        "for name, param in dktm.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print (name, param.data)"
      ],
      "metadata": {
        "id": "Op3YtHqS6BPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}